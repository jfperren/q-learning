{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from common import (\n",
    "    Discretizer,\n",
    ")\n",
    "from observers import (\n",
    "    StateAnalysisLogger,\n",
    "    WindowMetricLogger,\n",
    ")\n",
    "from strategies import (\n",
    "    EpsilonDecreasingStrategy\n",
    ")\n",
    "from training import (\n",
    "    DiscreteQLearningTrainer,\n",
    "    DiscreteQLearningTrainingConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julienperrenoud/anaconda3/envs/rl/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "# 1. Create environment\n",
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Setup Strategy & Training Config\n",
    "strategy = EpsilonDecreasingStrategy(epsilon=0.2)\n",
    "training_config = DiscreteQLearningTrainingConfig(\n",
    "    learning_rate=0.1,\n",
    "    discount=0.9,\n",
    "    episodes=2500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Discretize Space\n",
    "discretizer = Discretizer(env, [0.1, 0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Add Observers\n",
    "mean_reward_logger = WindowMetricLogger(window_size=50, metric='reward')\n",
    "state_analysis = StateAnalysisLogger(env=env, discretizer=discretizer, frequency=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Put it all together\n",
    "model = DiscreteQLearningTrainer(\n",
    "    env=env, \n",
    "    discretizer=discretizer, \n",
    "    training_config=training_config,\n",
    "    strategy=strategy,\n",
    "    observers=[\n",
    "        mean_reward_logger,\n",
    "        state_analysis\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training / Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Window reward: -180.88\n",
      "Visitation Pct: 0.6608187134502924\n",
      "Epoch: 50 | Window reward: -167.54\n",
      "Epoch: 100 | Window reward: -163.8\n",
      "Visitation Pct: 0.6608187134502924\n",
      "Epoch: 150 | Window reward: -172.64\n",
      "Epoch: 200 | Window reward: -166.12\n",
      "Visitation Pct: 0.6608187134502924\n",
      "Epoch: 250 | Window reward: -158.28\n",
      "Epoch: 300 | Window reward: -174.16\n",
      "Visitation Pct: 0.6608187134502924\n",
      "Epoch: 350 | Window reward: -173.0\n",
      "Epoch: 400 | Window reward: -157.2\n",
      "Visitation Pct: 0.6608187134502924\n",
      "Epoch: 450 | Window reward: -151.12\n",
      "Epoch: 500 | Window reward: -159.36\n",
      "Visitation Pct: 0.6608187134502924\n",
      "Epoch: 550 | Window reward: -160.28\n",
      "Epoch: 600 | Window reward: -170.94\n",
      "Visitation Pct: 0.6608187134502924\n",
      "Epoch: 650 | Window reward: -160.4\n",
      "Epoch: 700 | Window reward: -164.34\n",
      "Visitation Pct: 0.6608187134502924\n",
      "Epoch: 750 | Window reward: -166.76\n",
      "Epoch: 800 | Window reward: -163.02\n",
      "Visitation Pct: 0.6608187134502924\n",
      "Epoch: 850 | Window reward: -160.5\n",
      "Epoch: 900 | Window reward: -164.02\n",
      "Visitation Pct: 0.6608187134502924\n",
      "Epoch: 950 | Window reward: -163.24\n",
      "Epoch: 1000 | Window reward: -163.46\n",
      "Visitation Pct: 0.6608187134502924\n",
      "Epoch: 1050 | Window reward: -156.86\n",
      "Epoch: 1100 | Window reward: -152.8\n",
      "Visitation Pct: 0.6608187134502924\n",
      "Epoch: 1150 | Window reward: -152.92\n",
      "Epoch: 1200 | Window reward: -152.18\n",
      "Visitation Pct: 0.6608187134502924\n",
      "Epoch: 1250 | Window reward: -164.54\n",
      "Epoch: 1300 | Window reward: -170.12\n",
      "Visitation Pct: 0.6608187134502924\n",
      "Epoch: 1350 | Window reward: -165.16\n",
      "Epoch: 1400 | Window reward: -172.1\n",
      "Visitation Pct: 0.6608187134502924\n",
      "Epoch: 1450 | Window reward: -160.22\n",
      "Epoch: 1500 | Window reward: -167.54\n",
      "Visitation Pct: 0.6608187134502924\n",
      "Epoch: 1550 | Window reward: -156.02\n",
      "Epoch: 1600 | Window reward: -166.84\n",
      "Visitation Pct: 0.6608187134502924\n",
      "Epoch: 1650 | Window reward: -165.4\n",
      "Epoch: 1700 | Window reward: -152.96\n",
      "Visitation Pct: 0.6608187134502924\n",
      "Epoch: 1750 | Window reward: -153.12\n",
      "Epoch: 1800 | Window reward: -154.94\n",
      "Visitation Pct: 0.6619883040935672\n",
      "Epoch: 1850 | Window reward: -152.38\n",
      "Epoch: 1900 | Window reward: -169.7\n",
      "Visitation Pct: 0.6619883040935672\n",
      "Epoch: 1950 | Window reward: -151.08\n",
      "Epoch: 2000 | Window reward: -150.36\n",
      "Visitation Pct: 0.6619883040935672\n",
      "Epoch: 2050 | Window reward: -149.44\n",
      "Epoch: 2100 | Window reward: -168.2\n",
      "Visitation Pct: 0.6619883040935672\n",
      "Epoch: 2150 | Window reward: -149.42\n",
      "Epoch: 2200 | Window reward: -145.76\n",
      "Visitation Pct: 0.6631578947368421\n",
      "Epoch: 2250 | Window reward: -146.42\n",
      "Epoch: 2300 | Window reward: -150.8\n",
      "Visitation Pct: 0.664327485380117\n",
      "Epoch: 2350 | Window reward: -145.62\n",
      "Epoch: 2400 | Window reward: -149.4\n",
      "Visitation Pct: 0.664327485380117\n",
      "Epoch: 2450 | Window reward: -150.0\n"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl]",
   "language": "python",
   "name": "conda-env-rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
