{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from observers import (\n",
    "    WindowMetricLogger,\n",
    "    WindowStepMetricLogger,\n",
    "    StateAnalysisLogger,\n",
    "    TensorboardScalarLogger\n",
    ")\n",
    "from agents import (\n",
    "    DQNAgent,\n",
    "    EpsilonDecreasingStrategy\n",
    ")\n",
    "from training import (\n",
    "    QLearningTrainer,\n",
    "    QLearningContext,\n",
    "    episode_value_accessor\n",
    ")\n",
    "from common import (\n",
    "    Discretizer,\n",
    "    Tensorboard\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julienperrenoud/anaconda3/envs/rl/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=128\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(env.observation_space.shape[0], 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, env.action_space.n)\n",
    ")\n",
    "\n",
    "agent = DQNAgent(\n",
    "    env=env, \n",
    "    strategy=EpsilonDecreasingStrategy(\n",
    "        initial_epsilon=1.0,\n",
    "        min_epsilon=0.001,\n",
    "        decay=0.02\n",
    "    ),\n",
    "    model=model, \n",
    "    optimizer=optim.Adam(model.parameters(), lr=0.001),\n",
    "    loss=nn.MSELoss(), \n",
    "    discount=0.95,\n",
    "    memory_size=20000,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleWindowLogger():\n",
    "    \n",
    "    def __init__(self, name, apply):\n",
    "        self.name = name\n",
    "        self.apply = apply\n",
    "        \n",
    "    def on_train_start(self, context):\n",
    "        pass\n",
    "        \n",
    "    def on_step_end(self, context):\n",
    "        pass\n",
    "    \n",
    "    def on_episode_start(self, context):\n",
    "        pass\n",
    "    \n",
    "    def on_episode_end(self, context):\n",
    "        print(\"Epoch {epoch} | {name}={value}\".format(\n",
    "            epoch=context.get_episode_value('epoch'),\n",
    "            name=self.name,\n",
    "            value=self.apply(context)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorboardScalarLogger():\n",
    "    \n",
    "    def __init__(self, tb, name, apply):\n",
    "        self.tb = tb\n",
    "        self.name = name\n",
    "        self.apply = apply\n",
    "        \n",
    "    def on_train_start(self, context):\n",
    "        pass\n",
    "        \n",
    "    def on_step_end(self, context):\n",
    "        pass\n",
    "    \n",
    "    def on_episode_start(self, context):\n",
    "        pass\n",
    "    \n",
    "    def on_episode_end(self, context):\n",
    "        value = self.apply(context)\n",
    "        if value is not None:\n",
    "            self.tb.log_scalar(\n",
    "                self.name, \n",
    "                value,\n",
    "                context.get_episode_value('epoch')\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=128\n",
    "# HIDDEN_DIM=24\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(env.observation_space.shape[0], 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, env.action_space.n)\n",
    ")\n",
    "\n",
    "solver = DQNSolver(\n",
    "    env=env, \n",
    "    model=model, \n",
    "    optimizer=optim.Adam(model.parameters(), lr=0.001),\n",
    "    loss=nn.MSELoss(), \n",
    "    discount=0.95,\n",
    "    memory_size=20000,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "#         self.gamma = 0.95    # discount rate\n",
    "#         self.epsilon = 1.0  # exploration rate\n",
    "#         self.epsilon_min = 0.0001\n",
    "#         self.epsilon_decay = 0.999\n",
    "#         self.batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD_LOGDIR = \"./logs/cartpole-v0/2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_value(key):\n",
    "    return lambda c: c.get_episode_value(key) \n",
    "\n",
    "def episode_value_mean(key):\n",
    "    return lambda c: np.mean(c.get_episode_value(key)) if c.get_episode_value(key) is not None else None \n",
    "\n",
    "def episode_value_count(key):\n",
    "    return lambda c: len(c.get_episode_value(key)) if c.get_episode_value(key) is not None else None \n",
    "\n",
    "def episode_value_sum(key):\n",
    "    return lambda c: np.sum(c.get_episode_value(key)) if c.get_episode_value(key) is not None else None \n",
    "\n",
    "def train_observers():\n",
    "    tb = Tensorboard(TENSORBOARD_LOGDIR + '/' + dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "    return [\n",
    "        TensorboardScalarLogger(tb=tb, name='loss', apply=episode_value_mean('loss')),\n",
    "        TensorboardScalarLogger(tb=tb, name='target', apply=episode_value_mean('target')),\n",
    "        TensorboardScalarLogger(tb=tb, name='reward', apply=episode_value_sum('reward')),\n",
    "        TensorboardScalarLogger(tb=tb, name='epsilon', apply=episode_value('epsilon')),\n",
    "        TensorboardScalarLogger(tb=tb, name='action', apply=episode_value_mean('action')),\n",
    "#         TensorboardScalarLogger(tb=tb, name='epsilon', apply=lambda c: c.get_episode_value('epsilon')),\n",
    "#         TensorboardScalarLogger(tb=tb, name='target', apply=lambda c: np.mean(c.get_episode_value('target'))),\n",
    "#         TensorboardScalarLogger(tb=tb, name='reward', apply=lambda c: np.sum(c.get_episode_value('target')))\n",
    "#         SimpleWindowLogger(name='loss', apply=mean_episode_value('loss')),\n",
    "#         SimpleWindowLogger(name='loss_count', apply=episode_value_count('loss'))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Put it all together\n",
    "trainer = QLearningTrainer(\n",
    "    env=env, \n",
    "    solver=solver,\n",
    "    strategy=EpsilonDecreasingStrategy(\n",
    "        initial_epsilon=1.0,\n",
    "        min_epsilon=0.001,\n",
    "        decay=0.02\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir {TENSORBOARD_LOGDIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train(\n",
    "    epochs=1000,\n",
    "    observers=train_observers()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.forward(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.predict(state + [2.9, 0, 0, 0]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([s[0][0] for s in solver.memory])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([s[0][2] for s in solver.memory])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([s[0][1] for s in solver.memory])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([s[0][3] for s in solver.memory])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = copy.deepcopy(predictions.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(batch):\n",
    "    future_reward = solver.model.forward(torch.Tensor(item[3])).detach().max().item()\n",
    "    target[i][item[1]] = item[2] + solver.discount * future_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(target, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(torch.Tensor([[1, 1, 1, 1], [2, 1, 1, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.predict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.reshape(state, [1, self.env.observation_space.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.predict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_action(trainer, state):\n",
    "    action = np.argmax(trainer.model.forward(torch.Tensor(state)).detach()).item()\n",
    "    # print(action)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an action\n",
    "action = best_action(solver, state)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform update step, don't forget to discretize\n",
    "next_state, reward, done, _ = env.step(action)\n",
    "print(next_state, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = solver.model.forward(torch.Tensor(state))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_reward = solver.model.forward(torch.Tensor(next_state)).detach().max().item()\n",
    "print(future_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = copy.deepcopy(out.detach())\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[action] = reward + solver.discount * future_reward\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = solver.loss(out, target.detach())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "solver.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.model.forward(torch.Tensor(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target is the current Q-Value with new reward for action taken\n",
    "\n",
    "future_reward = self.model.forward(torch.Tensor(next_state)).detach().max().item()\n",
    "target = copy.deepcopy(out.detach())\n",
    "target[action] = reward + self.discount * future_reward\n",
    "\n",
    "# Move towards target with one backward pass\n",
    "loss = self.loss(out, target.detach())\n",
    "print(loss)\n",
    "loss.backward()\n",
    "self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(model.forward(torch.Tensor(env.reset())).detach()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(torch.Tensor(env.reset())).detach().max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(model.forward(torch.Tensor(env.reset())).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(torch.Tensor(env.reset())).detach()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = model.forward(torch.Tensor(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(torch.Tensor(env.reset())).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(torch.Tensor([1,2])).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "def train_model():\n",
    "  \n",
    "    model = create_model()\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    logdir = os.path.join(logs_base_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "    model.fit(\n",
    "        x=x_train, \n",
    "        y=y_train, \n",
    "        epochs=5, \n",
    "        validation_data=(x_test, y_test), \n",
    "        callbacks=[tensorboard_callback]\n",
    "    )\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import Tensorboard\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tb = Tensorboard('./logs')\n",
    "tb.log_scalar('loss', 0.92, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.summary.FileWriter('./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.contrib.summary.FileWriter('./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.create_file_writer('./logs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "SCORES_CSV_PATH = \"./scores/scores.csv\"\n",
    "SCORES_PNG_PATH = \"./scores/scores.png\"\n",
    "SOLVED_CSV_PATH = \"./scores/solved.csv\"\n",
    "SOLVED_PNG_PATH = \"./scores/solved.png\"\n",
    "AVERAGE_SCORE_TO_SOLVE = 195\n",
    "CONSECUTIVE_RUNS_TO_SOLVE = 100\n",
    "\n",
    "\n",
    "class ScoreLogger:\n",
    "\n",
    "    def __init__(self, env_name):\n",
    "        self.scores = deque(maxlen=CONSECUTIVE_RUNS_TO_SOLVE)\n",
    "        self.env_name = env_name\n",
    "\n",
    "        if os.path.exists(SCORES_PNG_PATH):\n",
    "            os.remove(SCORES_PNG_PATH)\n",
    "        if os.path.exists(SCORES_CSV_PATH):\n",
    "            os.remove(SCORES_CSV_PATH)\n",
    "\n",
    "    def add_score(self, score, run):\n",
    "        self._save_csv(SCORES_CSV_PATH, score)\n",
    "        self._save_png(input_path=SCORES_CSV_PATH,\n",
    "                       output_path=SCORES_PNG_PATH,\n",
    "                       x_label=\"runs\",\n",
    "                       y_label=\"scores\",\n",
    "                       average_of_n_last=CONSECUTIVE_RUNS_TO_SOLVE,\n",
    "                       show_goal=True,\n",
    "                       show_trend=True,\n",
    "                       show_legend=True)\n",
    "        self.scores.append(score)\n",
    "        mean_score = mean(self.scores)\n",
    "        print(\"Scores: (min: \" + str(min(self.scores)) + \", avg: \" + str(mean_score) + \", max: \" + str(max(self.scores)) + \")\\n\")\n",
    "        if mean_score >= AVERAGE_SCORE_TO_SOLVE and len(self.scores) >= CONSECUTIVE_RUNS_TO_SOLVE:\n",
    "            solve_score = run-CONSECUTIVE_RUNS_TO_SOLVE\n",
    "            print(\"Solved in \" + str(solve_score) + \" runs, \" + str(run) + \" total runs.\")\n",
    "            self._save_csv(SOLVED_CSV_PATH, solve_score)\n",
    "            self._save_png(input_path=SOLVED_CSV_PATH,\n",
    "                           output_path=SOLVED_PNG_PATH,\n",
    "                           x_label=\"trials\",\n",
    "                           y_label=\"steps before solve\",\n",
    "                           average_of_n_last=None,\n",
    "                           show_goal=False,\n",
    "                           show_trend=False,\n",
    "                           show_legend=False)\n",
    "            exit()\n",
    "\n",
    "    def _save_png(self, input_path, output_path, x_label, y_label, average_of_n_last, show_goal, show_trend, show_legend):\n",
    "        x = []\n",
    "        y = []\n",
    "        with open(input_path, \"r\") as scores:\n",
    "            reader = csv.reader(scores)\n",
    "            data = list(reader)\n",
    "            for i in range(0, len(data)):\n",
    "                x.append(int(i))\n",
    "                y.append(int(data[i][0]))\n",
    "\n",
    "        plt.subplots()\n",
    "        plt.plot(x, y, label=\"score per run\")\n",
    "\n",
    "        average_range = average_of_n_last if average_of_n_last is not None else len(x)\n",
    "        plt.plot(x[-average_range:], [np.mean(y[-average_range:])] * len(y[-average_range:]), linestyle=\"--\", label=\"last \" + str(average_range) + \" runs average\")\n",
    "\n",
    "        if show_goal:\n",
    "            plt.plot(x, [AVERAGE_SCORE_TO_SOLVE] * len(x), linestyle=\":\", label=str(AVERAGE_SCORE_TO_SOLVE) + \" score average goal\")\n",
    "\n",
    "        if show_trend and len(x) > 1:\n",
    "            trend_x = x[1:]\n",
    "            z = np.polyfit(np.array(trend_x), np.array(y[1:]), 1)\n",
    "            p = np.poly1d(z)\n",
    "            plt.plot(trend_x, p(trend_x), linestyle=\"-.\",  label=\"trend\")\n",
    "\n",
    "        plt.title(self.env_name)\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "\n",
    "        if show_legend:\n",
    "            plt.legend(loc=\"upper left\")\n",
    "\n",
    "        plt.savefig(output_path, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "    def _save_csv(self, path, score):\n",
    "        if not os.path.exists(path):\n",
    "            with open(path, \"w\"):\n",
    "                pass\n",
    "        scores_file = open(path, \"a\")\n",
    "        with scores_file:\n",
    "            writer = csv.writer(scores_file)\n",
    "            writer.writerow([score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# from scores.score_logger import ScoreLogger\n",
    "\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "\n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995\n",
    "\n",
    "\n",
    "class DQNSolver:\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))\n",
    "        self.model.add(Dense(24, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        for state, action, reward, state_next, terminal in batch:\n",
    "            q_update = reward\n",
    "            if not terminal:\n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))\n",
    "            q_values = self.model.predict(state)\n",
    "            q_values[0][action] = q_update\n",
    "            self.model.fit(state, q_values, verbose=0)\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "\n",
    "def cartpole():\n",
    "    print('xx')\n",
    "    env = gym.make(ENV_NAME)\n",
    "    score_logger = ScoreLogger(ENV_NAME)\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "    dqn_solver = DQNSolver(observation_space, action_space)\n",
    "    run = 0\n",
    "    while True:\n",
    "        run += 1\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "        step = 0\n",
    "        while True:\n",
    "            print('step')\n",
    "            step += 1\n",
    "            #env.render()\n",
    "            action = dqn_solver.act(state)\n",
    "            state_next, reward, terminal, info = env.step(action)\n",
    "            reward = reward if not terminal else -reward\n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)\n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                print(\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))\n",
    "                score_logger.add_score(step, run)\n",
    "                break\n",
    "            dqn_solver.experience_replay()\n",
    "\n",
    "\n",
    "cartpole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Reshape, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Neural Network model for Deep Q Learning\n",
    "def OurModel(input_shape, action_space):\n",
    "    X_input = Input(input_shape)\n",
    "    X = X_input\n",
    "\n",
    "    # 'Dense' is the basic form of a neural network layer\n",
    "    # Input Layer of state size(4) and Hidden Layer with 512 nodes\n",
    "    X = Dense(512, input_shape=input_shape, activation=\"relu\")(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "\n",
    "    # Hidden layer with 256 nodes\n",
    "    X = Dense(256, activation=\"relu\")(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    # Hidden layer with 64 nodes\n",
    "    X = Dense(64, activation=\"relu\")(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    # Output Layer with # of actions: 2 nodes (left, right)\n",
    "    X = Dense(action_space, activation=\"linear\")(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X, name='CartPoleModel')\n",
    "    model.compile(loss='mse', optimizer=Adam())\n",
    "    \n",
    "    return model\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.EPISODES = 1000\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        \n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.0001\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.batch_size = 128\n",
    "\n",
    "        self.model = OurModel(input_shape=(self.state_size,), action_space = self.action_size)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state))\n",
    "\n",
    "    def replay(self):\n",
    "        x_batch, y_batch = [], []\n",
    "        # Randomly sample minibatch from the memory\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # make the agent to approximately map the current state to future discounted reward\n",
    "            # We'll call that y_target\n",
    "            y_target = self.model.predict(state)\n",
    "            # if done, make our target reward\n",
    "            if done:\n",
    "                y_target[0][action] = reward\n",
    "            else:\n",
    "                # predict the future discounted reward\n",
    "                y_target[0][action] = reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "            # append results to lists, that will be used for training\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "\n",
    "        # Train the Neural Network with batches\n",
    "        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def load(self, name):\n",
    "        self.model = load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "\n",
    "    def run(self):\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                if not done:\n",
    "                    reward = reward\n",
    "                else:\n",
    "                    reward = -10\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                i += 1\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, self.EPISODES, i, self.epsilon))\n",
    "                    if i == 500:\n",
    "                        print(\"Saving trained model as cartpole-dqn.h5\")\n",
    "                        self.save(\"cartpole-dqn.h5\")\n",
    "                    break\n",
    "                self.replay()\n",
    "\n",
    "    def test(self):\n",
    "        self.load(\"cartpole-dqn.h5\")\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.model.predict(state))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = np.reshape(next_state, [1, self.state_size])\n",
    "                i += 1\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, i))\n",
    "                    break\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "agent = DQNAgent()\n",
    "agent.run()\n",
    "    #agent.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl]",
   "language": "python",
   "name": "conda-env-rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
